<?xml version="1.0" encoding="utf-8"?>
<ul>
  <li>链表反转对于链表的反转，一般有两种方法，递归和迭代，递归比较难以理解，并且效率没有迭代的高，但是写着帅，可以装X。递归的链表翻转第一步，整个翻转：classSolution{publicListNodereverseList(ListNodehead){//只聚焦第一次递归，定义本函数内的空间为本次元，递归中的空间为其他次元//basecase，这里返回的本次元的最后一个节点（翻转之后会变成头节点）if(head==null||head.next==null)returnhead;//递归返回的节点，在其他次元已经变成了头节点，但是在本次元还是尾节点，所以用last表示ListNodelast=reverseList(head.next);//此刻在其他次元head后面的已经完成了翻转，但是在本次元还没有翻转，所以在此时需要进行拼接，翻转后这个次元的head.next在上代码的次元是最后一个，故要在head.next后面接上本结点headhead.next.next=head;//同理，翻转之后head的后面应该是空head.next=null;//返回翻转后的头节点returnlast;}}第二步（1），翻转前n个节点：classSolution{//对比全部翻转，翻转前n个节点，需要记录下n+1个节点，好方便拼接，所以这里需要定义一个next记录ListNodenext;ListNodereverseN(ListNodehead,intn){//basecase，对比全部翻转，翻转前n个节点的停止条件变成n==1，同时记录下下一个节点if(n==1){next=head.next;returnhead;}//递归时，注意n-1，因为少了本次元的节点ListNodelast=reverseN(head.next,n-1);head.next.next=head;//对比全部翻转，这里要注意的是，翻转之后head的后面应该是nexthead.next=next;returnlast;}}第二步（2），翻转到某一节点：classSolution{ListNodenext;ListNodereverseN(ListNodehead,ListNodetail){if(head==tail){next=head.next;returnhead;}ListNodelast=reverseN(head.next,tail);head.next.next=head;head.next=next;returnlast;}}第三步，翻转一个区间：//先写出翻转前n个节点，再推进到可以翻转的点那，把翻转一个区间转变成翻转前n个节点classSolution{ListNodenext;publicListNodereverseBetween(ListNodehead,intleft,intright){//left==1，对应翻转前n个节点问题，此时返回的是翻转后的头节点if(left==1){returnreverseN(head,right);}//left不等于1时，则往后递归，同时处理好顺序关系，让head后面接递归后的结果（也就是翻转好的返回值，即下一次元的头节点）head.next=reverseBetween(head.next,left-1,right-1);//返回值是本次元的头节点returnhead;}ListNodereverseN(ListNodehead,intn){if(n==1){next=head.next;returnhead;}ListNodelast=reverseN(head.next,n-1);head.next.next=head;head.next=next;returnlast;}}迭代的链表翻转第一步，整个翻转：classSolution{publicListNodereverseList(ListNodehead){//对于链表，虚拟头节点可以使头节点变成非头节点，减少了逻辑判断，很棒ListNodedummy=newListNode(0);//对于不同的题，dummy后面接的会有不同，是接null还是head，需考虑题意dummy.next=null;ListNodenext=null;//头插法，翻转整个链表while(head!=null){next=head.next;head.next=dummy.next;dummy.next=head;head=next;}returndummy.next;}}第二步（1），翻转前n个节点：classSolution{publicListNodereverseN(ListNodehead,intn){ListNodedummy=newListNode(0);dummy.next=null;//对比翻转全部，此处添加计数器countintcount=0;//对比翻转全部，此处添加记录头节点的引用last，因为翻转后本次元的头节点会变成翻转部分的尾节点，为之后的拼接做准备ListNodelast=head;ListNodenext=null;//对比翻转全部，此处终止条件修改成count&lt;nwhile(count&lt;n){count++;next=head.next;head.next=dummy.next;dummy.next=head;head=next;}//最后拼接整个链接last.next=next;returndummy.next;}}第二步（2），翻转到某一结点：classSolution{publicListNodereverseList(ListNodehead,ListNodetail){//对于链表，虚拟头节点可以使头节点变成非头节点，减少了逻辑判断，很棒ListNodedummy=newListNode(0);//对于不同的题，dummy后面接的会有不同，是接null还是head，需考虑题意dummy.next=null;//记录下头节点ListNodelast=head;ListNodenext=null;//头插法，翻转整个链表while(head!=tail){next=head.next;head.next=dummy.next;dummy.next=head;head=next;}last.next=next;returndummy.next;}}第三步，翻转一个区间：classSolution{publicListNodereverseBetween(ListNodehead,intleft,intright){//本题dummy后面接的是headListNodedummy=newListNode(0,head);//要记录前一节点，所以多一个prevListNodeprev=dummy;//找到可以进行翻转前n个节点的那个节点，并记录该节点的前一节点while(left&gt;1){left--;right--;head=head.next;prev=prev.next;}//拼接链表prev.next=reverseN(head,right);returndummy.next;}publicListNodereverseN(ListNodehead,intn){ListNodedummy=newListNode(0);dummy.next=null;intcount=0ListNodelast=head;ListNodenext=null;while(count&lt;n){count++;next=head.next;head.next=dummy.next;dummy.next=head;head=next;}last.next=next;returndummy.next;}}反转链表反转链表IIK个一组翻转链表本题算是翻转一个区间的进阶版本，大概就是两个部分，第一部分是翻转前n个节点（或者翻转到某一结点），第二部分是主函数，可以是递归（类似翻转一个区间），也可以是迭代，下面用迭代完成代码（跟乐高一样，可以拼接）classSolution{publicListNodereverseKGroup(ListNodehead,intk){ListNodedummy=newListNode(0,head);ListNodeprev=dummy;ListNodetail=head;while(head!=null){for(inti=0;i&lt;k;i++){if(tail==null){returndummy.next;}tail=tail.next;}prev.next=reverseN(head,tail);prev=head;head=tail;}returndummy.next;}ListNodereverseN(ListNodehead,ListNodetail){ListNodedummy=newListNode(0);dummy.next=null;ListNodelast=head;ListNodenext=null;while(head!=tail){next=head.next;head.next=dummy.next;dummy.next=head;head=next;}last.next=next;returndummy.next;}}回文链表classSolution{ListNodeleft;publicbooleanisPalindrome(ListNodehead){left=head;returntraverse(head);}booleantraverse(ListNoderight){//类似二叉树的后序遍历if(right==null)returntrue;booleanres=traverse(right.next);res=res&amp;&amp;(right.val==left.val);left=left.next;returnres;}}classSolution{publicbooleanisPalindrome(ListNodehead){ListNodeslow,fast;slow=fast=head;while(fast!=null&amp;&amp;fast.next!=null){slow=slow.next;fast=fast.next.next;}if(fast!=null){slow=slow.next;}ListNodeleft=head;ListNoderight=reverse(slow);while(right!=null){if(left.val!=right.val)returnfalse;left=left.next;right=right.next;}returntrue;}//这里不需要拼接起来ListNodereverse(ListNodehead,ListNodetail){ListNodedummy=newListNode(0);dummy.next=null;ListNodenext=null;while(head!=tail){next=head.next;head.next=dummy.next;dummy.next=head;head=next;}returndummy.next;}}Reference：https://labuladong.gitee.io/algo/https://leetcode-cn.com/problems/</li>
  <li>图的介绍：https://labuladong.gitee.io/algo/2/18/26/图类似多叉树所有可能的路径//回溯求解classSolution{List&lt;List&lt;Integer&gt;&gt;res=newLinkedList&lt;&gt;();publicList&lt;List&lt;Integer&gt;&gt;allPathsSourceTarget(int[][]graph){LinkedList&lt;Integer&gt;path=newLinkedList&lt;&gt;();traverse(graph,0,path);returnres;}voidtraverse(int[][]graph,introot,LinkedList&lt;Integer&gt;path){path.add(root);//跳出条件intn=graph.length;if(root==n-1){res.add(newLinkedList&lt;&gt;(path));path.removeLast();return;}//遍历sfor(intpoint:graph[root]){traverse(graph,point,path);}path.removeLast();}}Reference：https://labuladong.gitee.io/algo/</li>
  <li>快速排序就是个二叉树的前序遍历，归并排序就是个二叉树的后序遍历操作二叉树关于二叉树的操作，无非是BFS和DFS（应该），大体的框架也可以总结出来//DFSclassSolution{publicEfunction(Eroot){if(root==null)returnroot;Eleft=function(root.left);Eright=function(root.right);//此处是一些操作，位置可以变，在后面就是后序遍历，其他位置同理returnroot;}}//BFS不分层classSolution{publicEfunction(Eroot){if(root==null)returnroot;Queue&lt;E&gt;queue=newLinkedList&lt;&gt;();queue.offer(root);while(!queue.isEmpty()){Etmp=queue.poll();//此处是一些操作if(tmp.left!=null)queue.offer(tmp.left);if(tmp.right!=null)queue.offer(tmp.right);}returnroot;}}//BFS分层classSolution{publicEfunction(Eroot){if(root==null)returnroot;Queue&lt;E&gt;queue=newLinkedList&lt;&gt;();queue.offer(root);while(!queue.isEmpty()){//对比不分层，此处多了一个计数intsize=queue.size();for(inti=0;i&lt;size;i++){Etmp=queue.poll();//此处是一些操作if(tmp.left!=null)queue.offer(tmp.left);if(tmp.right!=null)queue.offer(tmp.right);}}returnroot;}}翻转二叉树//前序遍历递归DFSclassSolution{publicTreeNodeinvertTree(TreeNoderoot){if(root==null)returnroot;TreeNodeleft=invertTree(root.left);TreeNoderight=invertTree(root.right);root.left=right;root.right=left;returnroot;}}//迭代BFSclassSolution{publicTreeNodeinvertTree(TreeNoderoot){if(root==null)returnroot;Queue&lt;TreeNode&gt;queue=newLinkedList&lt;&gt;();queue.offer(root);while(!queue.isEmpty()){TreeNodetmp=queue.poll();TreeNodeleft=tmp.left;TreeNoderight=tmp.right;tmp.left=right;tmp.right=left;if(left!=null)queue.offer(left);if(right!=null)queue.offer(right);}returnroot;}}填充每个节点的下一个右侧节点指针对于一般的题来说（对于我），BFS相对来说更好写出来，因为迭代更好理解一点，但是递归写出来就感觉，有大侠之姿（仅仅是帅，一直压栈也不支持）//前序遍历DFSclassSolution{publicNodeconnect(Noderoot){if(root==null)returnnull;connectTwoNode(root.left,root.right);returnroot;}//主要是考虑两个节点怎么连接起来，这样可以兼顾到两颗不同子树上的节点的连接问题voidconnectTwoNode(Nodenode1,Nodenode2){if(node1==null||node2==null)return;node1.next=node2;connectTwoNode(node1.left,node1.right);connectTwoNode(node2.left,node2.right);connectTwoNode(node1.right,node2.left);}}//BFS没什么好说的，对着模板，抄抄就行classSolution{publicNodeconnect(Noderoot){if(root==null)returnroot;Queue&lt;Node&gt;queue=newLinkedList&lt;&gt;();queue.offer(root);while(!queue.isEmpty()){intsize=queue.size();for(inti=0;i&lt;size;i++){Nodetmp=queue.poll();//核心业务段，嗯，一句话if(i&lt;size-1)tmp.next=queue.peek();if(tmp.left!=null)queue.offer(tmp.left);if(tmp.right!=null)queue.offer(tmp.right);}}returnroot;}}二叉树转链表在二叉树中做DFS，大概分两种，一种是设置个全局变量，一个是函数传参，模板其实跟上面也一样//设置全局变量，大力出奇迹，奥里给classSolution{Queue&lt;TreeNode&gt;queue=newLinkedList&lt;&gt;();publicvoidfunction(Eroot){if(root==null)return;dfs(root);//此处是操作}//把结点放入队列中voiddfs(Eroot){if(root==null)return;//在前面就是前序遍历queue.offer(root);dfs(root.left);dfs(root.right);}}//函数传参，优雅点classSolution{publicvoidfunction(Eroot){if(root==null)return;Queue&lt;E&gt;queue=newLinkedList&lt;&gt;();queue=dfs(root,queue);//此处是操作}Queue&lt;E&gt;dfs(Eroot,Queue&lt;E&gt;queue){if(root==null)returnnewLinkedList(queue);//在前面就是前序遍历queue.offer(root);dfs(root.left,queue);dfs(root.right,queue);returnnewLinkedList(queue);}}二叉树展开为链表用BFS来做这题，首先肯定还是需要DFS整个链表的，得到顺序的链表后再去一个一个设置left和right的值；用DFS来做，稍微不好想一点，最后要把做拼接操作//使用BFS，大力出奇迹，但是空间复杂度不是O（1），但是好想出来啊，附庸风雅的用传参的形式classSolution{publicvoidflatten(TreeNoderoot){if(root==null)return;Queue&lt;TreeNode&gt;queue=newLinkedList&lt;&gt;();queue=dfs(root,queue);intsize=queue.size();for(inti=0;i&lt;size-1;i++){TreeNodetmp=queue.poll();tmp.left=null;tmp.right=queue.peek();}}Queue&lt;TreeNode&gt;dfs(TreeNoderoot,Queue&lt;TreeNode&gt;queue){if(root==null)returnnewLinkedList(queue);queue.offer(root);dfs(root.left,queue);dfs(root.right,queue);returnnewLinkedList(queue);}}//递归，后序遍历classSolution{publicvoidflatten(TreeNoderoot){if(root==null)return;flatten(root.left);flatten(root.right);//递归中，左右子树已经拉平TreeNodeleft=root.left;TreeNoderight=root.right;//把左子树归位root.left=null;root.right=left;//把右子树归位TreeNodetmp=root;while(tmp.right!=null){tmp=tmp.right;}//把左子树和右子树拼接起来tmp.right=right;}}链表转二叉树构造一个二叉树，主要是找到根节点位置，借此将原链表分成两半，再分而治之最大二叉树构造最大二叉树，首先找到根节点，也就是最大值的索引，然后分成两半，各自递归，再合并，完事。要注意的是需要构建一个helper函数，来帮助定义上下界classSolution{publicTreeNodeconstructMaximumBinaryTree(int[]nums){returnbuild(nums,0,nums.length-1);}TreeNodebuild(int[]nums,intleft,intright){if(left&gt;right)returnnull;//找到数组最大值和最大值索引intindex=-1,maxVal=Integer.MIN_VALUE;for(inti=left;i&lt;=right;i++){if(maxVal&lt;nums[i]){index=i;maxVal=nums[i];}}TreeNoderoot=newTreeNode(maxVal);root.left=build(nums,left,index-1);root.right=build(nums,index+1,right);returnroot;}}从前序与中序遍历序列构造二叉树先将中序遍历的链表存入indexMap中，与上题的区别是上题找位置是一个一个比较找到值，而本题是用前序遍历第一个值就是根节点的特性，大体框架是一样的classSolution{privateMap&lt;Integer,Integer&gt;indexMap;publicTreeNodebuildTree(int[]preorder,int[]inorder){intn=preorder.length;indexMap=newHashMap&lt;&gt;();for(inti=0;i&lt;n;i++){indexMap.put(inorder[i],i);}returnmyBuildTree(preorder,inorder,0,n-1,0,n-1);}publicTreeNodemyBuildTree(int[]preorder,int[]inorder,intpreorder_left,intpreorder_right,intinorder_left,intinorder_right){if(preorder_left&gt;preorder_right)returnnull;intpreorder_root=preorder_left;intinorder_root=indexMap.get(preorder[preorder_root]);intsize_left_subtree=inorder_root-inorder_left;TreeNoderoot=newTreeNode(preorder[preorder_root]);root.left=myBuildTree(preorder,inorder,preorder_left+1,preorder_left+size_left_subtree,inorder_left,inorder_root-1);root.right=myBuildTree(preorder,inorder,preorder_left+size_left_subtree+1,preorder_right,inorder_root+1,inorder_right);returnroot;}}//https://leetcode-cn.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/solution/cong-qian-xu-yu-zhong-xu-bian-li-xu-lie-gou-zao-9/从中序与后序遍历序列构造二叉树思路同上题classSolution{privateMap&lt;Integer,Integer&gt;indexMap;publicTreeNodebuildTree(int[]inorder,int[]postorder){intn=postorder.length;indexMap=newHashMap&lt;&gt;();for(inti=0;i&lt;n;i++){indexMap.put(inorder[i],i);}returnmyBuildTree(postorder,inorder,0,n-1,0,n-1);}publicTreeNodemyBuildTree(int[]postorder,int[]inorder,intpostorder_left,intpostorder_right,intinorder_left,intinorder_right){if(postorder_left&gt;postorder_right)returnnull;intpostorder_root=postorder_right;intinorder_root=indexMap.get(postorder[postorder_root]);intsize_left_subtree=inorder_root-inorder_left;TreeNoderoot=newTreeNode(postorder[postorder_root]);//与上题区别在于边界root.left=myBuildTree(postorder,inorder,postorder_left,postorder_left+size_left_subtree-1,inorder_left,inorder_root-1);root.right=myBuildTree(postorder,inorder,postorder_left+size_left_subtree,postorder_right-1,inorder_root+1,inorder_right);returnroot;}}寻找重复的子树大体思路是每个节点的形状转换成String存储在map中，然后递归对比每个节点，如果map已经有了就说明两个节点是重复子树的头节点classSolution{HashMap&lt;String,Integer&gt;map=newHashMap&lt;&gt;();LinkedList&lt;TreeNode&gt;list=newLinkedList&lt;&gt;();publicList&lt;TreeNode&gt;findDuplicateSubtrees(TreeNoderoot){traverse(root);returnlist;}Stringtraverse(TreeNoderoot){if(root==null)return"#";Stringleft=traverse(root.left);Stringright=traverse(root.right);Stringcompare=left+","+right+","+root.val;map.put(compare,map.getOrDefault(compare,0)+1);if(map.get(compare)==2){list.add(root);}returncompare;}}//https://leetcode-cn.com/problems/find-duplicate-subtrees/solution/xun-zhao-zhong-fu-de-zi-shu-by-leetcode/高频面试题二叉树的最近公共祖先//核心是找该结点下的子树有没有pq结点classSolution{publicTreeNodelowestCommonAncestor(TreeNoderoot,TreeNodep,TreeNodeq){if(root==null||root==p||root==q)returnroot;TreeNodeleft=lowestCommonAncestor(root.left,p,q);TreeNoderight=lowestCommonAncestor(root.right,p,q);if(left!=null&amp;&amp;right!=null)returnroot;elseif(left!=null)returnleft;elsereturnright;}}//K神的剑指offerReference：https://labuladong.gitee.io/algohttps://leetcode-cn.com/problems/</li>
  <li>s求完全二叉树的节点个数，有两种做法，第一种是当成二叉树一个一个求节点个数；第二种是求左右子树的高度，因为已知是完全二叉树，所以到最后必然是左右子树高度一致完全二叉树的节点个数classSolution{publicintcountNodes(TreeNoderoot){TreeNodeleft=root,right=root;intleftCount=0,rightCount=0;while(left!=null){left=left.left;leftCount++;}while(right!=null){right=right.right;rightCount++;}if(leftCount==rightCount){return(int)Math.pow(2,leftCount)-1;}return1+countNodes(root.left)+countNodes(root.right);}}Reference：https://labuladong.gitee.io/algo/</li>
  <li>二叉搜索树（BinarySearchTree，BST）首先，BST的特性大家应该都很熟悉了：1、对于BST的每一个节点node，左子树节点的值都比node的值要小，右子树节点的值都比node的值大。2、对于BST的每一个节点node，它的左侧子树和右侧子树都是BST。二叉搜索树并不算复杂，直接基于BST的数据结构有AVL树，红黑树等等，拥有了自平衡性质，可以提供logN级别的增删查改效率；还有B+树，线段树等结构都是基于BST的思想来设计的。从做算法题的角度来看BST，除了它的定义，还有一个重要的性质：BST的中序遍历结果是有序的（升序）二叉搜索树转链表二叉搜索树转链表，之后再操作，对比二叉树转链表，多的一个特性就是中序遍历是有序的。模板类似于二叉树的转换，不过是把Queue换成ArrayList二叉搜索树中第K小的元素//中序遍历，把值放入list表中，再取出对应的值，粗鄙版本classSolution{ArrayList&lt;Integer&gt;list=newArrayList&lt;&gt;();publicintkthSmallest(TreeNoderoot,intk){traverse(root);returnlist.get(k-1);}voidtraverse(TreeNoderoot){if(root==null)return;traverse(root.left);list.add(root.val);traverse(root.right);}}//优雅版本classSolution{publicintkthSmallest(TreeNoderoot,intk){ArrayList&lt;Integer&gt;list=newArrayList&lt;&gt;();list=traverse(root,list);returnlist.get(k-1);}ArrayList&lt;Integer&gt;traverse(TreeNoderoot,ArrayList&lt;Integer&gt;list){if(root==null)returnlist;traverse(root.left,list);list.add(root.val);traverse(root.right,list);returnlist;}}//进阶版，粗鄙版本，达到添加就早停classSolution{intcount=0;intres;publicintkthSmallest(TreeNoderoot,intk){traverse(root,k);returnres;}voidtraverse(TreeNoderoot,intk){if(root==null)return;traverse(root.left,k);count++;if(count==k)res=root.val;elsetraverse(root.right,k);return;}}//优雅版本没想出来//迭代，先将左子树压入栈，再判断是否达到条件，不满足条件从最下面节点再压入右子树，比较难想出来classSolution{publicintkthSmallest(TreeNoderoot,intk){Stack&lt;TreeNode&gt;stack=newStack&lt;&gt;();while(true){while(root!=null){stack.push(root);root=root.left;}root=stack.pop();if(--k==0)returnroot.val;root=root.right;}}}二叉搜索树的累加把二叉搜索树转换为累加树//粗鄙版本classSolution{intsum=0;publicTreeNodeconvertBST(TreeNoderoot){traverse(root);returnroot;}voidtraverse(TreeNoderoot){if(root==null)return;traverse(root.right);root.val=root.val+sum;sum=root.val;traverse(root.left);}}//优雅版本classSolution{publicTreeNodeconvertBST(TreeNoderoot){traverse(root,0);returnroot;}inttraverse(TreeNoderoot,intsum){if(root==null)returnsum;//获取右子树（最左边）的sumintright=traverse(root.right,sum);//更新节点的值root.val+=right;//为了代码一致加的sum=root.val;//这里需要注意下，按照题意，函数要返回的值应该是也就是上一个次元需要的值，应该是本节点最左边的节点对应的sum，还是粗鄙版本好些，果然优雅都是大佬的事情intleft=traverse(root.left,sum);returnleft;}}//https://leetcode-cn.com/problems/convert-bst-to-greater-tree/solution/ba-er-cha-sou-suo-shu-zhuan-huan-wei-lei-jia-sh-14/评论热评二叉搜索树的增删查二叉搜索树中的插入操作//如果该子树不为空，则问题转化成了将val插入到对应子树上。//否则，在此处新建一个以val为值的节点，并链接到其父节点root上classSolution{publicTreeNodeinsertIntoBST(TreeNoderoot,intval){if(root==null)returnnewTreeNode(val);if(val&gt;root.val){root.right=insertIntoBST(root.right,val);}else{root.left=insertIntoBST(root.left,val);}}}//https://leetcode-cn.com/problems/insert-into-a-binary-search-tree/solution/er-cha-sou-suo-shu-zhong-de-cha-ru-cao-zuo-by-le-3/删除二叉搜索树中的节点classSolution{publicTreeNodedeleteNode(TreeNoderoot,intkey){if(root==null){//3.递归终止条件returnnull;}if(key&gt;root.val){//4.如果查找的结点比根节点大，继续在右子树查找删除该结点root.right=deleteNode(root.right,key);}elseif(key&lt;root.val){//5.如果查找的结点比根节点小，继续在左子树查找删除该结点root.left=deleteNode(root.left,key);}else{//6.如果找到了该结点，删除它if(root.left==null&amp;&amp;root.right==null){//7.以叶子节点为根节点的二叉搜索树只有一个元素，可以直接删除。root=null;}elseif(root.left==null){//8.如果有右子树，只要找到该右子树的最小值来替换，之后将它删除即可。root.val=rightMin(root);root.right=deleteNode(root.right,root.val);//9.将这个右子树的最小值替换根节点，此时存在两个相同节点，将这个节点删除即可。}else{//10.如果有左子树，只要找到该左子树的最大值来替换，之后将它删除即可。root.val=leftMax(root);root.left=deleteNode(root.left,root.val);//9.将这个左子树的最大值替换根节点，此时存在两个相同节点，将这个节点删除即可。}}returnroot;}publicintrightMin(TreeNoderoot){//1.找到以某个结点为根节点的右子树最小值。root=root.right;while(root.left!=null)root=root.left;returnroot.val;}publicintleftMax(TreeNoderoot){//2.找到以某个结点为根节点的左子树最大值。root=root.left;while(root.right!=null)root=root.right;returnroot.val;}}//https://leetcode-cn.com/problems/delete-node-in-a-bst/solution/shan-chu-er-cha-sou-suo-shu-zhong-de-jie-dian-by-l/评论热评二叉搜索树中的搜索//递归classSolution{publicTreeNodesearchBST(TreeNoderoot,intval){if(root==null||root.val==val)returnroot;if(val&gt;root.val){returnsearchBST(root.right,val);}else{returnsearchBST(root.left,val);}}}//https://leetcode-cn.com/problems/search-in-a-binary-search-tree/solution/er-cha-sou-suo-shu-zhong-de-sou-suo-by-leetcode///迭代classSolution{publicTreeNodesearchBST(TreeNoderoot,intval){while(root!=null&amp;&amp;root.val!=val){root=root.val&lt;val?root.right:root.left;}returnroot;}}验证是不是二叉搜索树有两种做法，第一种是验证树的值是否在一个区间内；第二种做法是中序遍历，看是不是递增的验证二叉搜索树//递归，判断子树的值是否在一个区间内classSolution{publicbooleanisValidBST(TreeNoderoot){returnhelper(root,Long.MIN_VALUE,Long.MAX_VALUE);}booleanhelper(TreeNoderoot,longlower,longupper){if(root==null)returntrue;if(root.val&lt;=lower||root.val&gt;=upper){returnfalse;}returnhelper(root.left,lower,root.val)&amp;&amp;helper(root.right,root.val,upper);}}//https://leetcode-cn.com/problems/validate-binary-search-tree/solution/yan-zheng-er-cha-sou-suo-shu-by-leetcode-solution///中序遍历classSolution{longpre=Long.MIN_VALUE;publicbooleanisValidBST(TreeNoderoot){returninorder(root);}booleaninorder(TreeNoderoot){if(root==null)returntrue;booleanleft=inorder(root.left);if(root.val&lt;=pre)returnfalse;pre=root.val;booleanright=inorder(root.right);returnleft&amp;&amp;right;}}//评论求有多少不同的二叉搜索树不同的二叉搜索树//DP//这里的G[0]=1表示，在后续的算F(i,n)中，左子树或者右子树为空也算一种情况//G[1]=1表示，只有单个结点classSolution{publicintnumTrees(intn){int[]G=newint[n+1];G[0]=1;G[1]=1;for(inti=2;i&lt;=n;i++){for(intj=1;j&lt;=i;j++){G[i]+=G[j-1]*G[i-j];}}returnG[n];}}//https://leetcode-cn.com/problems/unique-binary-search-trees/solution/bu-tong-de-er-cha-sou-suo-shu-by-leetcode-solution/不同的二叉搜索树II//递归获得所有可行的左子树和可行的右子树，最后一步只要从可行左子树集合中选一棵，再从可行右子树集合中选一棵拼接到根节点上，并将生成的二叉搜索树放入答案数组即可。classSolution{publicList&lt;TreeNode&gt;generateTrees(intn){if(n==0)returnnewLinkedList&lt;TreeNode&gt;();returnhelper(1,n);}List&lt;TreeNode&gt;helper(intstart,intend){List&lt;TreeNode&gt;allTrees=newLinkedList&lt;&gt;();if(start&gt;end){allTrees.add(null);returnallTrees;}for(inti=start;i&lt;=end;i++){List&lt;TreeNode&gt;leftTrees=helper(start,i-1);List&lt;TreeNode&gt;rightTrees=helper(i+1,end);for(TreeNodeleft:leftTrees){for(TreeNoderight:rightTrees){TreeNodecurrTree=newTreeNode(i);currTree.left=left;currTree.right=right;allTrees.add(currTree);}}}returnallTrees;}}//https://leetcode-cn.com/problems/unique-binary-search-trees-ii/solution/bu-tong-de-er-cha-sou-suo-shu-ii-by-leetcode-solut/乐高组合范例二叉搜索子树的最大键值和//先判断是不是BST，再求BST和，再进行比较求出最大键值和classSolution{intmaxSum=0;publicintmaxSumBST(TreeNoderoot){helper(root);returnmaxSum;}voidhelper(TreeNoderoot){if(isBST(root,Integer.MIN_VALUE,Integer.MAX_VALUE)){sumNodeValue(root);return;}helper(root.left);helper(root.right);}intsumNodeValue(TreeNoderoot){if(root==null)return0;intleft=sumNodeValue(root.left);intright=sumNodeValue(root.right);intsum=left+right+root.val;if(sum&gt;maxSum){maxSum=sum;}returnsum;}booleanisBST(TreeNoderoot,intmin,intmax){if(root==null)returntrue;if(root.val&lt;=min||root.val&gt;=max)returnfalse;returnisBST(root.left,min,root.val)&amp;&amp;isBST(root.right,root.val,max);}}Reference：https://labuladong.gitee.io/algo/https://leetcode-cn.com/problems/</li>
  <li>Java操作摘录于《labuladong的算法小抄》数组intm=5,n=10;//初始化一个大小为10的int数组//其中的值默认初始化为0int[]nums=newint[n];//初始化一个m*n的二维布尔数组//其中的元素默认值初始化为falseboolean[][]visited=newboolean[m][n];//长度nums.length;字符串StringStrings1="helloworld";//获取s1[2]中的字符charc=s1.charAt(2);char[]chars=s1.toCharArray();chars[1]='a';Strings2=newString(chars);//output:halloworldSystem.out.println(s2);//注意，用equals方法判断字符串是否相同if(s1.equals(s2)){//相同}else{//不同}//字符串可以用加号拼接Strings3=s1+'!';//output:helloworld!System.out.println(s3);//加号拼接不推荐，使用StringBuilderStringBuildersb=newStringBuilder();for(charc='a';c&lt;'f';c++){sb.append(c);}//append方法支持拼接字符、字符串、数字等类型sb.append('a').append("hij").append(123);Stringres=sb.toString();//output:abcedfghij123System.out.println(res);动态数组ArrayList//初始化一个储存String类型数据的动态数组ArrayList&lt;String&gt;nums=newArrayList&lt;&gt;();//初始化一个储存Integer类型数据的动态数组ArrayList&lt;Integer&gt;nums=newArrayList&lt;&gt;();//常用的方法：//判断数组是否为空booleanisEmpty();//返回数组中元素的个数intsize();//返回所有index的元素Eget(intindex);//在数组尾部添加元素ebooleanadd(Ee);双链表LinkedList//初始化一个存储String类型数据的双链表LinkedList&lt;String&gt;strings=newLinkedList&lt;&gt;();//常用方法：//判断链表是否为空booleanisEmpty();//返回链表中元素的个数intsize();//判断链表中是否存在元素obooleancontains(Objecto);//在链表尾部添加元素ebooleanadd(Ee);//在链表头部添加元素evoidaddFirst(Ee);//删除链表头部第一个元素EremoveFirst();//删除链表尾部最后一个元素EremoveLast();哈希表HashMap//初始化，整数映射到字符串的哈希表HashMap&lt;Integer,String&gt;map=newHashMap&lt;&gt;();//字符串映射到数组的哈希表HashMap&lt;String,int[]&gt;map=newHashMap&lt;&gt;();//常用方法：//判断哈希表中是否存在键booleancontainsKey(Objectkey);//获取键key对应的值，若key不存在，则返回nullVget(Objectkey);//将key和value键值对存入哈希表Vput(Kkey,Vvalue);//如果key存在，删除key并返回对应的值Vremove(Objectkey);//获得key的值，如果key不存在，返回defaultValueVgetOrDefault(Objectkey,VdefaultValue);//获取哈希表中的所有keySet&lt;K&gt;keySet();//如果key不存在，则将键值对key和value存入哈希表//如果key存在，则什么都不做VputIfAbsent(Kkey,Vvalue);哈希集合HashSet//新建一个存储String的哈希集合Set&lt;String&gt;set=newHashSet&lt;&gt;();//常用方法：//如果e不存在，则将e添加到哈希集合booleanadd(Ee);//判断元素o是否在哈希集合中booleancontains(Objecto);//如果元素o存在，则删除元素obooleanremove(Objecto);队列Queue//新建一个存储String的队列Queue&lt;String&gt;q=newLinkedList&lt;&gt;();//常用方法：//判断队列是否为空booleanisEmpty();//返回队列中元素的个数intsize();//返回队头的元素Epeek();//删除并返回队头的元素Epoll();//将元素e插入队尾booleanoffer(Ee);堆栈Stack//初始化Stack&lt;Integer&gt;stack=newStack&lt;&gt;();//常用方法：//判断堆栈是否为空booleanisEmpty();//返回堆栈中元素的个数intsize();//将元素压入栈顶Epush(Eitem);//返回栈顶元素Epeek();//删除并返回栈顶元素Epop();</li>
  <li>朴素贝叶斯法摘录自《统计学习方法》朴素贝叶斯（NaiveBayes）法是基于贝叶斯定理和特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。朴素贝叶斯法实现简单，学习与预测效率都很高。朴素贝叶斯法的学习与分类基本方法设输入空间$\mathcal{X}\subseteq\mathrm{R}^n$为$n$维向量的集合，输出空间为类标记集合$\mathcal{Y}={c_1,c_2,…,c_K}$。输入的特征向量$x\in\mathcal{X}$，输出为类标记（classlabel）$y\in\mathcal{Y}$。$X$是定义在输入空间$\mathcal{X}$上的随机向量，$Y$是定义在输出空间$\mathcal{Y}$上的随机变量。$P(X,Y)$是$X$和$Y$的联合概率分布。训练数据集：由$P(X,Y)$独立同分布产生。朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$，具体的，学习以下先验概率分布及条件概率分布。先验概率分布：条件概率分布：由于条件概率分布$P(X=x|Y=c_k)$有指数级数量的参数，所以朴素贝叶斯法对条件概率分布做了条件独立性的假设：朴素贝叶斯法实际上学习到生成数据的机制，属于生成模型。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使朴素贝叶斯法简单，但也会损失一定分类准确度。朴素贝叶斯法分类时，对给定的输入$x$，通过学习到的模型计算后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为$x$的类输出。后验概率：于是朴素贝叶斯分类器可表示为：由于所求的$y_{c_k}$分母相同，所以上式可化简为：后验概率最大化含义朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化。假设选择0-1损失函数：其中，$f(X)$是分类决策函数这时，期望风险函数为：期望是对联合分布$P(X,Y)$取得，由此取条件期望：为了使期望风险最小化，只需要对$X=x$逐个最小化，得：这样，根据期望风险最小化准则得到后验概率最大化准组（朴素贝叶斯法采用的原理）：朴素贝叶斯法的参数估计极大似然估计在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$。可以应用极大似然估计法估计相应的概率。先验概率$P(Y=c_k)$的极大似然估计是：设第$j$个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j2},…,a_{jS_j}}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：其中，$x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取得第$l$个值；$I$为指示函数学习与分类算法输入：训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，实例$x$输出：实例$x$的分类其中$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T$。$x_i^{(j)}$是第$i$个样本的第$j$个特征，$x_i^{(j)}\in{a_{j1},a_{j2},…,a_{jS_j}}$，$a_{jl}$是第$j$个特征可能取得第$l$个值，$j=1,2,..,n;l=1,2,…,S_j;y_i\in{c_1,c_2,…,c_K}$计算先验概率及条件概率：对于给定的实例$x=(x^{(1)},x^{(2)},…,x^{(n)})^T$，计算：确定实例$x$的类：贝叶斯估计用极大似然估计可能会出现所要估计的概率值为$0$的情况，这会影响后验概率的计算结果，使分类产生偏差。为解决这一问题，可使用贝叶斯估计：其中，$\lambda\geqslant0$等价于在随机变量各个取值的频数上赋予一个正数$\lambda&gt;0$，当$\lambda=0$时就是极大似然估计。常取$\lambda=1$，这时叫拉普拉斯平滑（Laplaciansmoothing）。显然，对任何$l=1,2,…,S_j,k=1,2,…,K$，有：同样，先验概率的贝叶斯估计是：</li>
  <li>支持向量机（SVM）本文摘录《统计学习方法》SVM简介支持向量机（supportvectormachines，SVM）是一种二分类模型。SVM的基础模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它称为实质上的非线性分类器。SVM的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的学习算法就是求解凸二次规划的最优化算法线性可分SVM和硬间隔最大化线性可分SVM假设给定一个特征空间上线性可分的训练数据集：其中，$x_i$为第$i$个特征向量，$y_i$为类标记，当它等于$+1$时为正例；为$-1$时为负例线性可分SVM：给定可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为：以及相应的分类决策函数：称为线性可分SVMSVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示，$w\cdotx+b$即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。图来源于支持向量机（SVM）——原理篇函数间隔和几何间隔函数间隔：对于给定的训练数据$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为：定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即：函数间隔可以表示分类预测的正确性和确信度。由于成比例的改变$w$和$b$，超平面没有变化，但函数间隔会发生改变，故需要对$w$加些约束，如规范化，这时候函数间隔就变成了几何间隔几何间隔：对于给定的训练数据$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为：定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即：超平面$(w,b)$关于样本点$(x_i,j_i)$的几何间隔一般是实例点到超平面的带符号的距离（signeddistance），当样本点被超平面正确分类时就是实例点到超平面的距离。实际上这个距离就是我们所谓的支持向量到超平面的距离函数间隔和几何间隔有如下关系：间隔最大化间隔最大化：对训练数据集找到的集合间隔最大的超平面意味着已充分大的确信度对训练数据进行分类根据以上定义，SVM模型的求解最大分割超平面问题可以表示为以下约束最优化问题：上式表示希望最大化超平面$(w,b)$关于训练数据集的几何间隔$\gamma$，约束条件表示的是超平面$(w,b)$关于每个训练样本点的几何间隔至少是$\gamma$考虑几何间隔和函数间隔的关系式，可改写为：函数间隔$\hat\gamma$的取值并不影响最优化问题的解，所有可以取$\hat\gamma=1$，代入方程，由于最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，故可以化简为凸二次规划问题（convexquadraticprogramming）：凸优化问题指约束最优化问题：其中，目标函数$f(w)$和约束函数$g_i(w)$都是$\mathrm{R}^n$上连续可微的凸函数，约束函数$h_i(w)$是$\mathrm{R}^n$上仿射函数（满足$h(w)=a\cdotw+b$）线性可分SVM学习算法–最大间隔法输入：线性可分训练数据集：输出：最大间隔分离超平面和分类决策函数构造并求解约束最优化问题：由此得到分离超平面：分类决策函数：$sign$函数是符号函数，功能是取某个数的符号。逻辑回归中用$sigmoid$函数实现支持向量和间隔边界在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（supportvector）支持向量是使约束条件式等号成立的点，即：对于$y_i=+1$的正例点，支持向量在超平面：对于$y_i=-1$的负例点，支持向量在超平面：即在$H_1,H_2$上的点就是支持向量，$H_1,H_2$之间的距离称为间隔（margin），$H_1,H_2$称为间隔边界，间隔等于$\frac{2}{||w||}$。在决定分离平面时只有支持向量起作用。学习的对偶算法todo线性SVM和软间隔最大化线性SVM给定一个特征空间上的训练数据集：其中，$x_i$为第$i$个特征向量，$y_i$为$x_i$的类标记假设训练数据集中不是线性可分的，有一些特异点（outlier），将这些特异点去掉，剩下大部分的样本点组成的集合时线性可分的。线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于$1$的约束条件，所有可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\varepsilon_i\geqslant0$，使函数间隔加上松弛变量大于等于$1$，即：同时，对每个松弛变量$\varepsilon_i$，支付一个代价$\varepsilon_i$。目标函数从原来的$\frac{1}{2}||w||^2$变成：其中，$C&gt;0$称为惩罚参数，一般由应用问题决定，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚增小线性不可分的线性SVM的学习问题变成如下凸二次规划问题：2.也可以在优化的目标函数里增加一个对特异点的惩罚项，最常用的是$hinge$函数：即若样本点满足约束条件损失就是$0$，否则损失就是$1-z$，则优化目标变成：惩罚参数$C$对于不同的惩罚参数$C$，SVM结果如下：本小段来源于看了这篇文章你还不懂SVM你就来打我原始目标函数：可抽象为：前一项可理解为“结构风险（structuralrisk）”，用来描述所求模型的某些性质（SVM就是要求间隔最大）；第二项称为“经验风险（empiricalrisk）”，用来描述模型与训练数据的契合程度（即误差）。而参数$C$就是用于对二者的折中，即我们一方面要求模型要满足某种性质另一方面又想使模型与训练数据很契合。从正则化角度来讲，$\Omega(f)$称为正则化项，$C$称为惩罚参数，$C$值大时对误分类的惩罚增大（要求模型对训练模型更契合），这可能会存在过拟合；$C$值小时对误分类的惩罚增小，即相对更加看重正则化项，此时可能存在欠拟合。学习的对偶算法todo支持向量todo合页损失函数todo非线性SVM和核函数前面介绍的都是线性问题，但是我们经常会遇到非线性的问题(例如异或问题)，此时就需要用到核技巧（kerneltrick）将线性支持向量机推广到非线性支持向量机。需要注意的是，不仅仅是SVM，很多线性模型都可以用核技巧推广到非线性模型，例如核线性判别分析（KLDA）核技巧用线性可分方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型图来源于看了这篇文章你还不懂SVM你就来打我核技巧应用到SVM，其基本思想就是通过一个非线性变换将输入空间（欧氏空间$\mathrm{R}^2$或离散集合）对应于一个特征空间（希尔伯特空间$\mathcal{H}$），使在输入空间$\mathrm{R}^2$中的超曲面模型对应于特征空间$\mathcal{H}$中的超平面模型（SVM）核技巧的定义：设$\mathcal{X}$是输入空间（欧氏空间$\mathrm{R}^n$的子集或离散集合），又设$\mathcal{H}$为特征空间（希尔伯特空间），如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射：使得对所有的$x,z\in\mathcal{X}$，函数$K(x,z)$满足条件：则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为两者内积（Innerproduct）核技巧的想法是，在学习和预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\phi$，这样计算比较容易。可以看到，对于给定的核$K(x,z)$，特征空间$\mathcal{H}$核映射函数$\phi$的取法不唯一，可以取不同的特征空间，即便在同一特征空间里也可以取不同的映射核技巧在SVM中的应用todo正定核充要条件证明todo常用核函数多项式核函数：对应的SVM是一个$p$次多项式分类器。在此情况下，分类决策函数称为：高斯核函数：对应的SVM是高斯径向基函数分类器。在此情况下，分类决策函数称为：字符串核函数：todo非线性SVMtodo序列最小最优算法（SMO）todo总结任何算法都有其优缺点，支持向量机也不例外。支持向量机的优点是:由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。不仅适用于线性线性问题还适用于非线性问题(用核技巧)。拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。理论基础比较完善(例如神经网络就更像一个黑盒子)。支持向量机的缺点是:二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数),因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)本小节来源于看了这篇文章你还不懂SVM你就来打我</li>
  <li>隐马尔可夫模型（HMM）本文摘抄自《统计学习方法》隐马尔可夫模型（HiddenMarkovModel,HMM）是可用于标注问题的统计学习模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。HMM的基本概念HMM是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测从而产生观测随机序列的过程。隐藏的马尔科夫链随机生成的状态的序列，称为状态序列（statesequence）；每个生成一个观测，而由此产生的观测的随机序列，称为观测序列（observationsequence）。序列的每一个位置又可以看作一个时刻。HMM由初始概率分布、状态转移概率分布和观测概率分布，HMM的形式定义如下：设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合：其中，$N$是可能的状态数，$M$是可能的观测数$I$是长度为$T$的状态序列，$O$是对应的观测序列：$A$是状态转移概率矩阵：其中，$a_{i,j}$是在时刻$t$处于状态$q_i$的条件下在时刻$t+1$转移到状态$q_j$的概率。$B$是观测概率矩阵：其中，$b_j(k)$是在时刻$t$处于状态$q_j$的条件下生成观测$v_k$的概率。$\pi$是初始状态概率向量：其中，$\pi_i$是时刻$t=1$处于状态$q_i$的概率。HMM由初始状态概率向量$\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。符号表示为：$A,B,\pi$称为HMM的三要素。由定义可知，HMM作了两个基本假设：齐次马尔可夫性假设：当前时刻的状态值，仅依赖于前一时刻的状态值，而不依赖于更早时刻的状态值（马尔可夫性），也与时刻无关（齐次性；即所有时刻共享一个状态转移矩阵）。观测独立性假设：当前时刻的观察值，仅依赖于当前时刻的状态值。HMM的三个基本问题:概率计算问题：给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O\lambda)$学习问题：已知观测序列$O=(o_1,o_2,…,o_T)$，估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O\lambda)$最大，即用极大似然估计的方法估计参数预测问题：也称为解码问题。已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率$P(IO)$最大的状态序列$I=(i_1,i_2,…,i_T)$，即给定观测序列，求最有可能的对应的状态序列概率计算算法给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O\lambda)$直接计算法（实际不可行）通过列举所有可能的长度为$T$的状态序列$I=(i_1,i_2,…,i_T)$，求各个状态序列$I$与观测序列$O=(o_1,o_2,…,o_T)$的联合概率$P(O,I\lambda)$，然后对所有可能的状态序列求和，得到$P(O\lambda)$状态序列$I=(i_1,i_2,…,i_T)$的概率是：对固定的状态序列$I=(i_1,i_2,…,i_T)$，观测序列$O=(o_1,o_2,…,o_T)$的概率是：$O$和$I$同时出现的联合概率是：然后对所有可能的状态序列$I$求和，得到观测序列$O$的概率$P(O|\lambda)$，即：但是上式计算量大（$O(TN^T)$），故可不行前向算法前向概率：给定HMM$\lambda$，定义到时刻$t$部分观测序列为$o_1,o_2,…,o_t$且状态为$q_i$的概率为前向概率，记作：输入：HMM$\lambda$，观测序列$O$输出：观测序列概率$P(O|\lambda)$初值递推，对$t=1,2,…,T-1$终止后向算法给定HMM$\lambda$，定义在时刻$t$状态为$q_i$的条件下，从$t+1$到$T$的部分观测序列为$o_{t+1},o_{t+2},…,o_T$的概率为后向概率，记作：输入：HMM$\lambda$，观测序列$O$输出：观测序列概率$P(O|\lambda)$初值递推，对$t=T-1,T-2,…,1$终止一些概率与期望的计算todo学习算法监督学习方法todoEM算法todo预测算法已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率$P(IO)$最大的状态序列$I=(i_1,i_2,…,i_T)$，即给定观测序列，求最有可能的对应的状态序列近似算法近似算法的思想是，在每个时刻$t$选择在时刻最有可能出现的状态$i_t^$，从而得到一个状态序列$I^=(i_1^,i_2^,…,i_T^*)$，将它作为预测的结果给定HMM$\lambda$和观测序列$O$，在时刻$t$处于状态$q_i$的概率$\gamma_t(i)$是在每一时刻$t$最有可能的状态$i_t^$是从而得到状态序列$I^=(i_1^,i_2^,…,i_T^*)$优点是计算简单，缺点是不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列有可能有实际上不发生的部分（状态转移概率为0，即对某些$i,j$有$a_{ij}=0$）。维特比算法（Viterbialgorithm）通俗解释Viterbialgorithm维特比算法实际是用动态规划解HMM模型的预测问题，即用动态规划求概率最大路径（最优路径），这时一条路径对应着一个状态序列。根据动态规划原理，最优路径具有这样特性：如果最优路径在时刻$t$通过结点$i_t^$，那么这一路径从结点$i_t^$到终点$i_T^$的部分路径，对于从$i_t^$到$i_T^$的所有可能的部分路径来说，必然是最优的。所以，我们只需从时刻$t=1$开始，递推的计算在时刻$t$状态为$i$的各条部分路径的最大概率，直至得到时刻$t=T$状态为$i$的各条路径的最大概率。时刻$t=T$状态的最大概率即为最优路径的概率$P^$，最优路径的终结点$i_T^$也同时得到。之后，为了找出最优路径的各个结点，从终结点$i_T^$开始，由后往前逐步求得结点$i_{T-1}^,…,i_1^$，得到最优路径$I^*=(i_1^,i_2^,…,i_T^*)$。首先导入两个变量$\delta$和$\psi$定义在时刻$t$状态为$i$的所有单个路径$(i_1,i_2,…,i_t)$中概率最大值为：由定义可得变量$\delta$的递推公式：定义在时刻$t$的状态为$i$的所有单个路径$(i_1,i_2,…,i_{t-1},i)$中概率最大的路径的第$t-1$个结点为输入：模型$\lambda=(A,B,\pi)$和观测$O=(o_1,o_2,…,o_T)$输出：最优路径$I^=(i_1^,i_2^,…,i_T^)$初始化递推，对$t=2,3,…,T$终止最优路径回溯，对$t=T-1,T-2,…,1$求得最优路径$I^*=(i_1^,i_2^,…,i_T^*)$</li>
  <li>条件随机场（CRF）图来自Anintroductiontoconditionalrandomfields本文摘抄自《统计学习方法》条件随机场（ConditionalRandomField,CRF）是给定一组输入随机变量条件下另一组输出随机条件变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫随机场。条件随机场可用于不同的预测问题，本书仅讨论它在标注问题的应用。因此主要讲述线性链（linearchain）条件随机场，这时，问题变成了由输入序列对输出序列预测的判别模型，形式为对数线性模型，其学习方法通常是极大似然估计或正则化的极大似然估计。概率无向图模型todoCRF的定义和形式CRF的定义CRF是给定随机变量$X$的条件下，随机变量$Y$的马尔可夫随机场，下面主要讲线性链条件随机场（linearchainconditionalrandomfield）CRF定义：设$X$和$Y$是随机变量，$P(Y|X)$是在给定$X$的条件下$Y$的条件概率分布，若随机变量$Y$构成一个由无向图$G=(V,E)$表示的马尔可夫随机场，即对于任意结点$v$成立，则称条件概率分布$P(Y|X)$为条件随机场。其中，$w\simv$表示在图$G=(V,E)$中与结点$v$有边连接的所有结点$w,w\nev$表示结点$v$以外的所有结点，$Y_v,Y_w$为结点$v,w$对应的随机变量LCCRF定义：设$X=(X_1,X_2,…,X_n),~Y=(Y_1,Y_2,…,Y_n)$均为线性链表示的随机变量序列，若在给定随机变量序列$X$的条件下，随机变量序列$Y$的条件概率分布$P(Y|X)$构成条件随机场，即满足马尔可夫性CRF的参数化形式LCCRF的参数化形式：设$P(Y|X)$为线性链条件随机场，则在随机变量$X$取值为$x$的条件下，随机变量$Y$取值为$y$的条件概率具体如下形式：其中，$t_k$和$s_l$是特征函数，$\lambda_k$和$\mu_l$是对应的权值。$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。上式表示给定输入序列$x$，对输出序列$y$预测的条件概率。其中$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前和前一个位置；$s_l$是定义在节点上的特征函数，称为状态特征，依赖于当前位置。$t_k$和$s_l$都依赖于位置，是局部特征函数。通常，特征函数$t_k$和$s_l$的取值为1或0；当满足特征条件时取值为1，否则为0。CRF的简化形式首先，将转移特征和状态特征及其权重用统一的符号表示。设有$K_1$个转移特征，$K_2$个状态特征，$K=K_1+K_2$，记：然后，对转移与状态特征在各个位置$i$求和，记：再用$w_k$表示特征$f_k(y,x)$的权值，即：于是简化为：若用$w$表示权重向量，即：以$F(y,x)$表示全局特征向量，即：则CRF可写成向量$w$和$F(y,x)$的内积的形式：CRF的矩阵形式对每个标记序列引进特殊的起点和终点状态标记$y_0=start$和$y_{n+1}=stop$，这时标注序列的概率$P_w(yx)$可以通过矩阵形式表示并有效计算对观测序列$x$的每一个位置$i=1,2,…,n+1$，由于$y_{i-1}$和$y_i$在$m$个标记中取值，可以定义一个$m$阶矩阵随机变量：矩阵随机变量的元素为：这样，给定观测序列$x$，相应标记序列$y$的非规范化概率可以通过该序列$n+1$个矩阵的适当元素的乘积$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示。于是，条件概率$P_w(y|x)$是：其中，规范化因子$Z_w(x)$是以$start$和$stop$为终点通过状态的所有路径$y_1y_2…y_n$的非规范化概率$\prod_{i=1}^{n+1}M_i(y_{i-1},y_ix)$之和CRF概率计算算法前向-后向算法对每个指标$i=0,1,…,n+1$，定义前向向量$\alpha_i(x)$：递推公式为：又可以表示为：其中，$\alpha_i(y_ix)$表示为位置$i$的标记是$y_i$并且从$1$到$i$的前部分标记序列的非规范化概率，$y_i$可取的值有$m$个，所以$\alpha_i(x)$是$m$维列向量同样，对每个指标$i=0,1,…,n+1$，定义后向向量$\beta_i(x)$：递推公式为：又可以表示为：其中，$\beta_i(y_ix)$表示在位置$i$的标记为$y_i$并且从$i+1$到$n$的后部分标记序列的非规范化概率概率计算todo期望值的计算todoCRF学习算法改进的迭代尺度法todo拟牛顿法todoCRF的预测算法维特比算法（Viterbialgorithm）CRF中的维特比算法则CRF可写成向量$w$和$F(y,x)$的内积的形式：由上式有：于是，CRF的预测问题可以转换成求非规范化概率最大的最优路径问题：这里，路径表示标记序列。其中：注意，这时只需计算非规范概率，而不必计算概率，可以大大提高效率。为了求解最优路径，可以将问题写成：其中，$F_i(y_{i-1},y_i,x)$是局部特征。==这一块不是很明白，为什么可以大大提高效率，因为分解开并行计算？还有非规范概率是什么意思？以后找到了答案补充==首先求出位置1的各个标记$j=1,2,…,m$的非规范化概率：一般的，由递推公式，求出到位置$i$的各个标记$l=1,2,…,m$的非规范化概率的最大值，同时记录非规范化概率最大值的路径：==上式中加法，我理解是因为原来的公式是用了exp将乘法转换成了加法==直到$i=n$时终止，这时求得非规范化概率的最大值为：及最优路径的终点：由此最优路径终点返回：求得最优路径$y^*=(y_1^,y_2^,…,y_n^*)^T$输入：模型特征向量$F(y,x)$和权值向量$w$，观测序列$x=(x_1,x_2,…,x_n)$输出：最优路径$y^*=(y_1^,y_2^,…,y_n^*)$初始化递推，对$i=2,3,…,n$终止最优路径回溯，对$i=n-1,n-2,…,1$求得最优路径$y^*=(y_1^,y_2^,…,y_n^*)$</li>
  <li>最近的梯度下降算法的更新概述对之前的总结和补充VanillaGradientDescent现代深度学习优化器的故事始于朴素梯度下降。朴素梯度下降遵循以下迭代，具有一定的学习速率参数$\eta$：其中loss是从整个训练数据集中随机抽取的、使用一定数量的样本计算得出的平均loss。对于批次梯度下降（或简称为梯度下降），在每次迭代中都使用整个训练数据集来计算损失。对于随机梯度下降（SGD），每次迭代绘制一个样本。在实践中，通常使用微型批处理，并且常见的微型批处理大小在64到2048之间。小批量梯度下降非常普遍，通常称为SGD。Momentum考虑两种情况，a）局部损失景观是平坦的山丘，b）局部损失景观是陡峭的山沟。在第一种情况下，梯度下降算法可能要花很长时间才能到达山顶，即使它显然沿相同方向行进。在第二种情况下，梯度下降算法可能会在陡峭的山沟壁之间来回弹跳而不会很快到达底部，如果可以对来回梯度进行平均以减小方差，那将是很好的。这是需要梯度累加或平均机制的两种常见解释，这导致了Momentum出现。Momentum是由$\beta$（通常等于0.9）参数化的过去梯度的指数移动平均值（exponentialmovingaverageofpastgradients），由以下算法给出：有多种实现方式见JamesMelvilleAdaGrad(AdaptiveGradient)AdaGrad受直觉启发，不经常更新的权重应该以比经常更新的权重更大的学习率来更新。基本上，如果权重很少得到大的梯度，则当它收到大的梯度时应该充分利用它。这是通过保持每个权重的平方梯度的累加总和并将学习率除以该累加总和来实现的。从AdaGrad开始，针对$\theta$的每个坐标$i$的SGD递归变为：其中$G_t\in\mathbb{R}^{p\timesp}$通常是过去梯度的平方之和的对角线预处理矩阵，而$\epsilon&gt;0$是一个小常数。注意在$G_t$上使用的平方根（也可以有效地使用0到1之间的其他幂），但是平方根是一个很好的起点。这通常有另外两个含义：a）对超参数不像Momentum那么敏感（某些模型一次训练可能要花费100,000美元！），b）这近似于对角线矩阵，并且是仅使用一阶矩信息对二阶距的估计。这些学习速率自适应算法可以理解为利用当前和过去的梯度信息来设计更好地近似损失函数的局部曲率的预处理矩阵。RMSprop但是，AdaGrad中平方梯度的累积只会增加，这意味着如果优化程序运行足够长的时间，则实际上后续更新太小。RMSprop（均方根反向传播）将每次累积的矩阵$G_t$替换为每次迭代计算的梯度均方根，其中$\beta_2$最初建议为0.9，良好的默认学习率为0.001。然后，RMSprop更新为：Adam如果我们为RMSprop增加动力该怎么办？这就是Adam。除了RMSprop之外，Adam还保留了过去梯度的指数衰减平均值：【另外一个博客说这边还要计算经过偏差校正的第一和第二矩估计值来抵消这些偏差】其中建议$\beta_1=0.9,\beta_2=0.999,学习率\eta=0.001$。当$\beta_1=0$时，观察到Adam等于RMSprop。我们再次提到要注意$E[g^2]_t$的平方根可以取别的值，并且0到1之间的其他幂已经被有效使用（有一种称为Padam的算法）。AMSGradAMSGrad的动机在于，Adam无法收敛于一个简单的优化问题。作者通过Adam证明中有关指数移动平均$E[g^2]_t$的技术细节来解决此问题。AMSGrad保留了$E[g^2]$的运行最大值，而不是指数移动平均值。参数化AMSGrad算法（$\eta=0,\beta_1&lt;1,\beta_2&lt;1$）:通常公认的是，AMSGrad在实践中并不比Adam更好。另外，没人真正知道AMSGrad代表什么。AdamW权重衰减（Weightdecay）是训练神经网络的一种技术，它试图使权重值保持较小。直觉是，权重过大往往会造成过拟合。这通常是通过在损失函数中添加一个项来实现的，该项是权重值的函数，这样，较大的权重将显着增加总损失。权重衰减的最流行形式是L2正则化，它惩罚权重的平方值，并且便于同时处理正负权重以及可微性。AdamW通过将权重衰减与梯度更新解耦，修改了Adam中权重衰减正则化的典型实现。特别是，通常使用以下修改实现Adam中的L2正则化，其中$w_t$是时间$t$的权重衰减率：而AdamW则将权重衰减项调整为出现在梯度更新中：事实证明，这在实践上有所作为，并且已在机器学习社区的某些部分采用。您会惊讶于一些小细节如何对性能产生显着影响！RefAnupdatedoverviewofrecentgradientdescentalgorithmsTobecontinue</li>
  <li>梯度下降优化算法总览优化指的是改变$\theta$以最小化或最大化某个函数$J(\theta)$的任务。我们通常以最小化$J(\theta)$指代大多数最优化问题。最大化可经由最小化算法最小化$-J(\theta)$来实现。我们把要最小化或最大化的函数称为目标函数（objectivefunction）或准则（criterion）。当我们对其进行最小化时，我们也把它称为代价函数（costfunction）、损失函数（lossfunction）或误差函数（errorfunction）。【花书】梯度下降是一种优化算法，其中，学习率$\eta$决定了我们到达（局部）最小值的步长梯度下降种类梯度下降根据计算目标函数的梯度所用的数据量不同，大体分为三类。根据数据量，我们可以在参数更新的准确性和执行更新所需的时间之间进行权衡BGD（批次梯度下降）公式：由于在每一次更新中，我们都需要计算整个数据集的梯度，所以BGD可能会非常缓慢，并且内存装不下的数据集很棘手。不可在线学习（需要一次得到整个数据集）。然后，我们以与梯度相反的方向更新参数，学习率决定了我们执行的更新量。对于凸面误差曲面，批梯度下降可以保证收敛到全局最小值，对于非凸面曲面，可以保证收敛到局部最小值。SGD（随机梯度下降）相较与BGD，SGD对每个训练样本$x^{(i)}$和标签$y^{(i)}$都做参数更新。【由于是做BN，都是一个batch一个batch的算，计算一个batch里的所有样本】公式：BGD对于大型数据集会有冗余计算，因为他会在每个参数更新之前重新计算相似样本的梯度。SGD通过一次执行一次更新的方式来消除这种冗余，因此它通常速度很快，也可以用于在线学习。SGD频繁执行更新且变化很大，这导致目标函数如下图所示，会剧烈波动当BGD收敛到参数的盆地的最小值时，SGD的波动一方面使它跳到新的并可能是更好的局部最小值，但另一方面，这也导致SGD会错过最低值（overshooting），使得SGD收敛到最小值会很困难。但是，已经表明，当我们缓慢降低学习率时，SGD会显示与BGD相同的收敛行为，几乎可以肯定地，对于非凸优化和凸优化，它们分别收敛到局部或全局最小值。MBGD（小批次梯度下降）MBGD是BGD和SGD的折中，每次更新小批量中的$n$个样本的梯度。公式：这样做1）减少了参数更新的方差，这可以使收敛更加稳定，2）可以利用深度学习库的高度优化的矩阵优化功能，快速的计算一个小批次数据的梯度。常见的小批次大小设置在50到256之间（随不同应用变化），训练神经网络时，通常选择小批量梯度下降算法，而当使用小批量时，通常也使用术语SGD。挑战但是，MBGD不能保证良好的收敛性，但是存在一些需要解决的问题：选择合适的学习率十分困难，学习率太小会导致收敛过于缓慢，学习率太大导致不容易收敛，并导致损失函数在最小值附近波动甚至发散；学习率schedules可以在训练过程中调整学习率比如说退火算法，即根据预定义的时间表或在各个时期之间的目标变化降到阈值以下时降低学习率。但是，这些计划和阈值必须预先定义，因此无法适应数据集的特征；此外，所有参数更新都使用同一学习率。如果我们的数据稀疏并且特征有非常不同的频率，那么我们可能不想所有的参数都更新相同的程度，而是对较少出现的特征进行更大的更更新；最小化神经网络常见的高度非凸误差函数的另一个关键挑战是避免陷入其众多次优局部最小值中。Dauphinetal.认为收敛困难实际上不是由局部极小值引起的，而是由鞍点（即一维向上倾斜而另一维向下倾斜的点）引起的。这些鞍点通常被相同误差的平稳段包围，这使得SGD很难逃脱，因为在所有维度上梯度都接近于零。梯度下降优化算法MomentumSGD很难在沟壑中前进，即在一个维度上，曲面的弯曲比另一个维度要陡得多，这在局部最优情况下很常见。在这种情况下SGD会在峡谷得山坡山震荡，犹豫不决得向局部最优方向前进，如下图。Momentum可以在相关方向加速SGD并抑制震荡，如下图。通过将过去时间步长得更新向量的分数$\gamma$与当前更新向量相加来实现。公式：其中，动量项$\gamma$通常设为0.9。需要注意的是一些实现交换方程式中的符号，如下：其他实现见JamesMelville比如，我们将球推下山坡，球在下坡时滚动时会积累动量，在途中速度会越来越快（如果存在空气阻力，即$\gamma$&lt;1，直到达到最终速度）。我们的参数更新也发生了同样的事情：动量项对于梯度指向相同方向的维增加，而对于梯度改变方向的维减少动量。结果，我们获得了更快的收敛并减少了振荡。NAG(NesterovAcceleratedGradient)然后令人不满的是让一个球从山上滚下来，会盲目的随着斜坡滚，我们希望这个球具有要往何处的概念，这样当山坡再次变高之前知道它应该减速。NAG完成了这种预见的功能【花书中解释是相当于加了一个校正向量】，我们先通过计算$\theta-\gammav_{t-1}$计算出参数下一个位置的近似值（离完整更新还缺少梯度），通过计算不是现在位置的参数$\theta$而是未来的位置参数$\theta$的梯度，从而达到有效率的前看（lookahead）。公式为：其中，动量项$\gamma$通常设为0.9Momentum首先计算当前梯度（图中的蓝色小矢量），然后在更新的累积梯度（蓝色矢量）的方向上发生较大的跃迁，而NAG首先在先前的累积梯度的方向上进行较大的跃迁（棕色矢量），测量梯度，然后进行校正（红色矢量），从而完成NAG更新（绿色矢量）。这种预期的更新可防止我们过快地进行，并导致响应速度增加，从而显着提高了RNN在许多任务上的性能。现在，我们能够使更新适应误差函数的斜率并依次提高SGD，我们还希望使更新适应每个单独的参数，以根据其重要性执行更大或更小的更新。另一种角度解释，这里使用另一种NAG的实现方程：对NAG原来的更新公式进行变换，得到这样的等效形式：可以看到这个NAG的等效形式与Momentum的区别在于，本次更新方向多加了一个$\gamma[g(\theta_t)-g(\theta_{t-1})]$，它的直观含义就很明显了：如果这次的梯度比上次的梯度变大了，那么有理由相信它会继续变大下去，那我就把预计要增大的部分提前加进来；如果相比上次变小了，也是类似的情况。这个多加上去的项不就是在近似目标函数的二阶导！所以NAG本质上是多考虑了目标函数的二阶导信息，怪不得可以加速收敛了！其实所谓“往前看”的说法，在牛顿法这样的二阶方法中也是经常提到的，比喻起来是说“往前看”，数学本质上则是利用了目标函数的二阶导信息。等价推导，NAG原始公式：两边同时减$\eta\gammav_{t+1}$有：可令：代入上式有：对$\hatv_{t+1}$展开可得：所以$\hatv_{t+1}-\gamma\hatv_t$有：证明成功，原作者太强了。ref:https://zhuanlan.zhihu.com/p/22810533AdagradAdagrad是一种基于梯度的优化算法，它可以使学习率适应参数，对与频繁出现的特征相关的参数执行较小的更新（即低学习率），对与不频繁出现的特征相关的参数执行较大的更新（即高学习率）。因此非常适合处理稀疏数据之前，我们一次对所有的参数$\theta$进行更新，因为每个参数$\theta_i$使用相同的学习率$\eta$。因为Adagrad在每个时间步长$t$对每个参数$\theta_i$使用不同的学习率，因此首先对显示Adagrad的每个参数更新，再将其向量化。为了简便起见，使用$g_t$表示时间步长$t$处的梯度，$g_{t,i}$表示时间步长$t$参数$\theta_i$的目标函数的偏导数：在SGD中，每个时间步长$t$处每个参数$\theta$的更新为：在Adagrad中，基于对$\theta_i$计算的过去梯度，针对每个参数$\theta_i$在每个时间步长$t$修改通用学习率$\eta$：其中，$G_t\in\mathbb{R}^{d\timesd}$，这是个对角矩阵，其中每个对角元素$i,i$是在时间步长$t$处$\theta_i$的梯度的平方和，$\epsilon$是一个平滑项（smoothingterm），避免分母为0（通常设置为$1e^{-8}$量级）。有趣的是，如果没有平方根运算，该算法的性能将大大降低由于$G_t$包括过去的沿对角线的所有参数$\theta$梯度的平方和，所有我们可以通过对$G_t$和$g_t$之间做矩阵向量点积$\odot$来向量化实现：Adagrad的主要好处之一是，它无需手动调整学习率，大多数实现使用默认的0.01Adagrad的主要弱点是分母中平方梯度的累加：由于每个加法项都是正数，所以累加和在训练期间不断增长。反过来，这导致学习率下降，并最终变得无限小，这时该算法不再能够获取其他知识。以下算法旨在解决此缺陷。AdadeltaAdadelta是Adagrad的扩展，旨在降低其激进的，单调降低的学习率。Adadelta不会累计所有过去的平方梯度，而是将累计过去的梯度的窗口限制为某个固定大小$w$。代替无效的存储过去平方梯度（pastsquaredgradients），将梯度之和递归定义为所有过去梯度梯度的衰减平均值（decayingaverage），在时间步长$t$处的移动平均值（runningaverage）$E[g^2]_t$仅取决与先前的平均值和当前梯度（分数$\gamma$相当于Momentum里的动量项）：我们将$\gamma$设置与动量项相似的值，约为0.9。为了清晰起见，先根据参数更新向量$\Delta\theta_t$来重写朴素SGD：因此，我们先前推导的Adagrad的参数更新向量采用以下形式：现在我们用过去梯度梯度上的衰减平均值（decayingaverage）代替对角矩阵$G_t$：由于分母只是梯度的均方差（RMS）误差准组，可以替换简写准则：作者注意到，此处更新的单位（以及SGD，Momentum或Adagrad中的单位）不匹配，即，更新应具有与参数相同的假设单位。为了实现这一点，他们首先定义了另一个指数衰减的平均值，这次不是平方梯度而是平方参数更新：由于$RMS[\Delta\theta]t$是未知的，因此我们使用参数更新的$RMS$对其进行近似，直到上一个时间步长为止。最终，用$RMS[\Delta\theta]{t-1}$代替先前的更新规则中的学习率$\eta$，得出Adadelta更新规则：所以使用Adadelta，我们甚至不需要设置默认的学习率，因为它已从更新规则中删除。RMSpropRMSprop是由GeoffHinton在他的Coursera课的第6e讲中提出的未公开的自适应学习率方法。RMSprop和Adadelta大约在同一时间提出，这是因为需要解决Adagrad的学习率急剧下降的问题。RMSprop实际上与我们上面得出的Adadelta的第一个更新向量相同：RMSprop也将学习率除以平方梯度的指数衰减平均值（exponentiallydecayingaverage）。Hinton建议将$\gamma$设置为0.9，学习率$\eta$的默认值是0.001。Adam(AdaptiveMomentEstimation)Adam是另一种计算每个参数的自适应学习率的方法。等价RMSprop+Momentum除了存储像Adadelta和RMSprop这样的过去平方梯度$v_t$的指数衰减平均值之外，Adam还保留了过去梯度$m_t$的指数衰减平均值，类似于Momentum。Momentum以看作是一个顺着斜坡滑下的球，而Adam的行为就像是一个带有摩擦的沉重的球，因此，它更喜欢在误差表面上保持平坦的最小值。我们分别计算过去梯度$m_t$和过去平方梯度$v_t$的衰减平均值，如下所示：$m_t$和$v_t$分别是梯度的第一矩（均值）和第二矩（无中心方差）的估计值。当$m_t$和$v_t$初始化为0的向量时，Adam的作者观察到它们偏向零，尤其是在初始时间步长，衰减率较小时（即β1和β2接近1）。他们通过计算经过偏差校正的第一和第二矩估计值来抵消这些偏差：然后，他们使用它们来更新参数，就像在Adadelta和RMSprop中所看到的那样，这将产生Adam更新规则：作者建议将$\beta_1$的默认值设置为0.9，将$\beta_2$的默认值设置为0.999，将$\epsilon$的默认值设置为$10^{-8}$。AdaMaxAdam的更新规则中，$v_t$与过去梯度（通过$v_{t-1}$）和当前梯度$|g_t|^2$的$l_2$范数成反比的缩放梯度：我们可以将此更新推广到$l_p$范数：大$p$值的范数通常在数值上变得不稳定，这就是为什么$l_1$和$l_2$范数在实践中最常见的原因。但是，$l_\infty$通常也表现出稳定的行为。因此，作者提出了AdaMax，并证明具有$l_\infty$的$v_t$收敛到以下更稳定的值。为了避免与Adam混淆，我们使用$u_t$表示无穷范数约束$v_t$：然后就可以将$u_t$代替$\sqrt{v_t}+\epsilon$插入Adam的更新方程中，得到AdaMax的更新规则：请注意，由于$u_t$依赖于$max$运算，因此不建议像Adam中的$m_t$和$v_t$那样偏向零，这就是为什么我们不需要计算$u_t$的偏差校正的原因。默认值是$\eta=0.002,\beta_1=0.9,\beta_2=0.999$。Nadam(Nesterov-acceleratedAdaptiveMomentEstimation)如之前所见，Adam可以看作是RMSprop和Momentum的组合：RMSprop贡献了过去平方梯度$v_t$的指数衰减平均值，而Momentum贡献了过去梯度$m_t$的指数衰减平均值。同时我们也看到NAG由于普通的Momentum。因此，Nadam结合了Adam和NAG。为了将NAG合并到Adam中，我们需要修改其动量项$m_t$。首先回顾Momentum的更新规则：其中$J$是我们的目标函数，$\gamma$是动量衰减项，$\eta$是我们的步长。将上面的第三个方程式扩展会得出：这再次证明了Momentum在前一个动量矢量的方向上迈出一步以及在当前梯度的方向上迈出一步。NAG允许我们在计算梯度之前通过用动量步长更新参数来在梯度方向上执行更精确的步长。因此，我们只需要修改梯度$g_t$即可得出NAG：Dozat建议以以下方式修改NAG：直接应用前瞻动量矢量（look-aheadmomentumvector）来更新当前参数，而不是用两此Momentum步骤（一次用于更新梯度$g_t$，第二次用于更新参数$\theta_{t+1}$）注意到，我们不再像上面的Momentum更新规则的公式那样使用先前的动量矢量$m_{t-1}$，而是现在使用当前的动量矢量$m_t$向前看。为了将Nesterov动量添加到Adam，我们可以类似地用当前动量矢量替换以前的动量矢量。首先，回想一下Adma更新规则如下（注意，我们不需要修改$\hatv_t$，所以此处省略$\hatv_t$的更新公式）：依次用$\hatm_t$和$m_t$的定义扩展第二个方程式，我们得到：注意到，$\frac{\beta_1m_{t-1}}{1-\beta_1^t}$只是前一时间步长的动量矢量的偏差校正估计，因此，我们可以将其替换为$m_{t-1}$：为简单起见，我们忽略了分母为$1-\beta_1^t$而不是$1-\beta_1^{t-1}$，因为无论如何我们将在下一步中替换分母。该方程式再次看起来与我们上面扩展的Momentum更新规则非常相似。现在，我们可以像以前一样添加Nesterov动量，只需用当前动量矢量$\hatm_{t-1}$的偏差校正后的估计值简单地替换前一个时间步$\hatm_t$的该偏差校正后的估计值即可，Nadam更新规则：AMSGrad由于自适应学习率方法已成为训练神经网络的规范，因此从业者注意到在某些情况下，例如对于对象识别或机器翻译，它们无法收敛到最佳，不如Momentum。Reddi等人正式解决了这个问题，指出过去平方梯度的指数移动平均值是自适应学习率方法的泛化行为不佳的原因之一。回想一下，引入指数平均值的动机很充分：应防止学习速度随着训练的进行而变得无穷小，而这是Adagrad算法的关键缺陷。但是，在其他情况下，这种对梯度的短期记忆成为一个障碍。在Adam收敛到次优解的环境中，已经观察到一些小型批次提供了较大且信息丰富的梯度，但是由于这些小型批次很少发生，因此指数平均会降低其影响，从而导致较差的收敛性。作者提供了一个简单的凸优化问题的示例，其中Adam可以观察到相同的行为。为了解决此问题，作者提出了一种新算法AMSGrad，该算法使用过去的平方梯度$v_t$的最大值而不是指数平均值来更新参数。$v_t$的定义与上面的Adam相同：现在，我们直接使用以前的$v_{t-1}$（如果它大于当前的$v_t$），而不是直接使用$v_t$（或其偏差校正版本$\hatv_t$）：这样，AMSGrad的步长不会增加，从而避免了Adam遇到的问题。为简单起见，作者还删除了我们在Adam中看到的去偏差步骤。可以看到完整的没有经过偏差校正的估计的AMSGrad更新：作者观察到在小型数据集和CIFAR-10上，与Adam相比，性能有所提高。但是，其他实验显示出与Adam相似或更差的性能。在实践中，AMSGrad是否能够始终胜过Adam，还有待观察。使用哪个优化算法？如果输入数据稀疏，可以使用一种自适应学习率方法来获得最佳结果。另一个好处是，您无需调整学习率，但可以使用默认值获得最佳结果。总而言之，RMSprop是Adagrad的扩展，用于处理其学习率从根本上降低的问题。它与Adadelta相同，除了Adadelta在分子更新规则中使用参数更新的RMS。最后，Adam为RMSprop添加了偏差校正和动量。Kingma等人表明，随着梯度变得稀疏，它的偏差校正有助于Adam在优化结束时略胜于RMSprop。就目前而言，Adam可能是最好的整体选择。优化SGD的其他策略最后，我们介绍了可以与前面提到的任何算法一起使用的其他策略，以进一步提高SGD的性能。Shuffling和课程学习通常，我们希望避免以有意义的顺序向我们的模型提供训练示例，因为这可能会使优化算法产生偏差。因此，在每个epoch之后将训练数据shuffling通常是一个好主意。另一方面，在某些情况下，我们旨在解决日益棘手的问题，以有意义的顺序提供训练样本实际上可能会导致性能提高和更好的融合。建立这种有意义的顺序的方法称为课程学习。Zaremba和Sutskever只能训练LSTM来使用课程学习评估简单的程序，并且表明联合或混合策略比单纯的策略更好，后者通过增加难度来对示例进行排序。Batchnormalization为了促进学习，我们通常通过使用零均值和单位方差初始化参数的初始值来对其进行归一化。随着训练的进行以及我们在不同程度上更新参数，我们将失去这种标准化，这会减慢训练速度，并随着网络变得更深而扩大变化。批量归一化为每个小批量重新建立这些归一化，并且更改也通过操作反向传播。通过将归一化作为模型体系结构的一部分，我们可以使用较高的学习率，而对初始化参数的关注较少。批处理规范化还可以充当正则化器，从而减少（有时甚至消除）对Dropout的需求。Earlystopping根据GeoffHinton的说法：“尽早停止是美丽的免费午餐”（NIPS2015教程幻灯片，幻灯片63）。因此，您应该始终在训练过程中监视验证集中的错误，如果验证错误没有得到足够的改善，则应停止（有耐心）。GradientnoiseNeelakantan等人将噪声遵循高斯分布$N(0,\sigma_t^2)$到每个梯度更新：他们根据以下schedule对方差进行退火：他们表明，添加这种噪声可使网络对不良的初始化更加健壮，并有助于训练特别深而复杂的网络。他们怀疑增加的噪声使模型有更多的机会逃脱并找到新的局部极小值，这对于更深层的模型而言更为常见。RefAnoverviewofgradientdescentoptimizationalgorithms（中文版）Anoverviewofgradientdescentoptimizationalgorithms</li>
  <li>标题这里是h1这里是h2这里是h3这里是h4这里是h5这里是h6#这里是h1##这里是h2###这里是h3####这里是h4#####这里是h5######这里是h6段落段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落一段落段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落二段落超链接TMaizeBlog[TMaizeBlog](http://blog.tmaize.net)引用这里是引用&gt;这里是引用常见字体样式斜体粗体删除线_斜体_**粗体**~~删除线~~列表无序列表1-1缩进2空格缩进2空格缩进2空格无序列表1-2无序列表1-3有序列表1-1缩进3空格缩进3空格缩进3空格有序列表1-2有序列表1-3-无序列表1-1缩进2空格-缩进2空格-缩进2空格-无序列表1-2-无序列表1-31.有序列表1-1缩进3空格1.缩进3空格2.缩进3空格2.有序列表1-23.有序列表1-3分割线---图片破事水滑稽![line](http://xx.com/xx.jpg)块级别图片![测试图片](001.jpg)代码行这是一段文字rm-rf/*这是一段文字这是一段文字`rm-rf/*`这是一段文字代码块blog.encodeHtml=function(html){varo=document.createElement('div')o.innerText=htmlvartemp=o.innerHTMLo=nullreturntemp}​```javascriptblog.encodeHtml=function(html){varo=document.createElement('div')o.innerText=htmlvartemp=o.innerHTMLo=nullreturntemp}```表格测试TablesAreCoolcol3isright-aligned$1600col2iscentered$12zebrastripesareneat$1​```md|Tables|Are|Cool||————-|:———–:|—–:||col3is|right-aligned|$1600||col2is|centered|$12||zebrastripes|areneat|$1|##数学公式需要在配置中开启这是一行话`\(\int_0^\infty\frac{x^3}{e^x-1}\,dx=\frac{\pi^4}{15}\)`这是一行话`\[\int_0^\infty\frac{x^3}{e^x-1}\,dx=\frac{\pi^4}{15}\]`##插入html&lt;divid="htmldemo"&gt;&lt;/div&gt;&lt;style&gt;#htmldemo{height:30px;width:30px;background-color:#00aa9a;animation-name:moveX;animation-duration:1s;animation-timing-function:linear;animation-iteration-count:infinite;animation-direction:alternate;animation-fill-mode:both;}@keyframesmoveX{0%{transform:translateX(0px);}100%{transform:translateX(100px);}}&lt;/style&gt;```html&lt;divid="htmldemo"&gt;&lt;/div&gt;&lt;style&gt;#htmldemo{height:30px;width:30px;background-color:#00aa9a;animation-name:moveX;animation-duration:1s;animation-timing-function:linear;animation-iteration-count:infinite;animation-direction:alternate;animation-fill-mode:both;}@keyframesmoveX{0%{transform:translateX(0px);}100%{transform:translateX(100px);}}&lt;/style&gt;插入iframe&lt;!--属性什么的不要错了，最好用双引号括住--&gt;&lt;!--网易云的iframe需要做些调整，调整如下--&gt;&lt;iframesrc="//music.163.com/outchain/player?type=2&amp;id=28445796&amp;auto=0&amp;height=66"frameborder="0"width="100%"height="86px"&gt;&lt;/iframe&gt;</li>
</ul>